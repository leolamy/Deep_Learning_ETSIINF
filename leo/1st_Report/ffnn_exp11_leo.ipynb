{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Assignment: Image recognition\n- Alumno 1:\n- Alumno 2:\n- Alumno 3:\n\nThe goals of the assignment are:\n* Develop proficiency in using Tensorflow/Keras for training Neural Nets (NNs).\n* Put into practice the acquired knowledge to optimize the parameters and architecture of a feedforward Neural Net (ffNN), in the context of an image recognition problem.\n* Put into practice NNs specially conceived for analysing images. Design and optimize the parameters of a Convolutional Neural Net (CNN) to deal with previous task.\n* Train popular architectures from scratch (e.g., GoogLeNet, VGG, ResNet, ...), and compare the results with the ones provided by their pre-trained versions using transfer learning.\n\nFollow the link below to download the classification data set  “xview_recognition”: [https://drive.upm.es/s/2DDPE2zHw5dbM3G](https://drive.upm.es/s/2DDPE2zHw5dbM3G)","metadata":{"editable":true,"id":"QYuALZOG-AMq","slideshow":{"slide_type":""},"tags":[]}},{"cell_type":"code","source":"import os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\nimport requests\nimport zipfile\n\nurl = 'https://drive.upm.es/s/2DDPE2zHw5dbM3G/download'\nzip_name = 'dataset.zip'\n\nr = requests.get(url, stream=True)\nwith open(zip_name, 'wb') as f:\n    for chunk in r.iter_content(chunk_size=1024):\n        f.write(chunk)\n\nif os.path.getsize(zip_name) < 10000:\n    print(f\"ERREUR : Le fichier {zip_name} est trop petit. Le lien est invalide ou nécessite une connexion.\")\nelse:\n    with zipfile.ZipFile(zip_name, 'r') as z:\n        z.extractall(\".\") \n\n    target_file = 'xview_ann_train.json'\n    found_path = None\n    \n    for root, dirs, files in os.walk(\".\"):\n        if target_file in files:\n            found_path = os.path.join(root, target_file)\n            break\n    \n    if found_path:\n        print(f\"SUCCÈS : Fichier trouvé à : {found_path}\")\n        \n        import json\n        json_file = found_path \n        \n        with open(json_file) as ifs:\n            json_data = json.load(ifs)\n        print(\"Base de données chargée avec succès !\")\n        \n    else:\n        print(f\"ERREUR : {target_file} reste introuvable après extraction.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T21:28:23.811327Z","iopub.execute_input":"2026-02-12T21:28:23.811654Z","iopub.status.idle":"2026-02-12T21:29:13.689930Z","shell.execute_reply.started":"2026-02-12T21:28:23.811628Z","shell.execute_reply":"2026-02-12T21:29:13.689091Z"}},"outputs":[{"name":"stdout","text":"SUCCÈS : Fichier trouvé à : ./xview_recognition/xview_ann_train.json\nBase de données chargée avec succès !\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import tensorflow as tf\nprint(tf.config.list_physical_devices('GPU'))","metadata":{"ExecuteTime":{"end_time":"2024-10-26T00:00:21.031186Z","start_time":"2024-10-26T00:00:17.131476Z"},"editable":true,"slideshow":{"slide_type":""},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T21:29:13.691215Z","iopub.execute_input":"2026-02-12T21:29:13.691467Z","iopub.status.idle":"2026-02-12T21:29:17.359723Z","shell.execute_reply.started":"2026-02-12T21:29:13.691445Z","shell.execute_reply":"2026-02-12T21:29:17.358862Z"}},"outputs":[{"name":"stderr","text":"2026-02-12 21:29:14.008570: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1770931754.031020  102710 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1770931754.037942  102710 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1770931754.059715  102710 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770931754.059744  102710 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770931754.059747  102710 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770931754.059749  102710 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import uuid\nimport numpy as np\n\nclass GenericObject:\n    \"\"\"\n    Generic object data.\n    \"\"\"\n    def __init__(self):\n        self.id = uuid.uuid4()\n        self.bb = (-1, -1, -1, -1)\n        self.category= -1\n        self.score = -1\n\nclass GenericImage:\n    \"\"\"\n    Generic image data.\n    \"\"\"\n    def __init__(self, filename):\n        self.filename = filename\n        self.tile = np.array([-1, -1, -1, -1])  # (pt_x, pt_y, pt_x+width, pt_y+height)\n        self.objects = list([])\n\n    def add_object(self, obj: GenericObject):\n        self.objects.append(obj)","metadata":{"ExecuteTime":{"end_time":"2024-10-26T00:00:21.066937Z","start_time":"2024-10-26T00:00:21.059126Z"},"editable":true,"id":"OYtqD3Oh-AMw","slideshow":{"slide_type":""},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T21:29:17.360786Z","iopub.execute_input":"2026-02-12T21:29:17.361366Z","iopub.status.idle":"2026-02-12T21:29:17.367332Z","shell.execute_reply.started":"2026-02-12T21:29:17.361330Z","shell.execute_reply":"2026-02-12T21:29:17.366672Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"categories = {0: 'Cargo plane', 1: 'Small car', 2: 'Bus', 3: 'Truck', 4: 'Motorboat', 5: 'Fishing vessel', 6: 'Dump truck', 7: 'Excavator', 8: 'Building', 9: 'Helipad', 10: 'Storage tank', 11: 'Shipping container', 12: 'Pylon'}","metadata":{"ExecuteTime":{"end_time":"2024-10-26T00:00:21.153693Z","start_time":"2024-10-26T00:00:21.149079Z"},"id":"I_GygShu-AMz","trusted":true,"execution":{"iopub.status.busy":"2026-02-12T21:29:17.368466Z","iopub.execute_input":"2026-02-12T21:29:17.369055Z","iopub.status.idle":"2026-02-12T21:29:17.384918Z","shell.execute_reply.started":"2026-02-12T21:29:17.369032Z","shell.execute_reply":"2026-02-12T21:29:17.384231Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"!pip install rasterio","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T21:29:17.386932Z","iopub.execute_input":"2026-02-12T21:29:17.387330Z","iopub.status.idle":"2026-02-12T21:29:20.483588Z","shell.execute_reply.started":"2026-02-12T21:29:17.387309Z","shell.execute_reply":"2026-02-12T21:29:20.482673Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: rasterio in /usr/local/lib/python3.12/dist-packages (1.5.0)\nRequirement already satisfied: affine in /usr/local/lib/python3.12/dist-packages (from rasterio) (2.4.0)\nRequirement already satisfied: attrs in /usr/local/lib/python3.12/dist-packages (from rasterio) (25.4.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from rasterio) (2026.1.4)\nRequirement already satisfied: click!=8.2.*,>=4.0 in /usr/local/lib/python3.12/dist-packages (from rasterio) (8.3.1)\nRequirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.12/dist-packages (from rasterio) (0.7.2)\nRequirement already satisfied: numpy>=2 in /usr/local/lib/python3.12/dist-packages (from rasterio) (2.0.2)\nRequirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from rasterio) (3.2.5)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import warnings\nimport rasterio\nimport numpy as np\n\ndef load_geoimage(filename):\n    warnings.filterwarnings('ignore', category=rasterio.errors.NotGeoreferencedWarning)\n    src_raster = rasterio.open('./xview_recognition/'+filename, 'r')\n    # RasterIO to OpenCV (see inconsistencies between libjpeg and libjpeg-turbo)\n    input_type = src_raster.profile['dtype']\n    input_channels = src_raster.count\n    img = np.zeros((src_raster.height, src_raster.width, src_raster.count), dtype=input_type)\n    for band in range(input_channels):\n        img[:, :, band] = src_raster.read(band+1)\n    return img","metadata":{"ExecuteTime":{"end_time":"2024-10-26T00:00:21.292654Z","start_time":"2024-10-26T00:00:21.205321Z"},"editable":true,"id":"fRBA7ReQ-AM0","slideshow":{"slide_type":""},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T21:29:20.484926Z","iopub.execute_input":"2026-02-12T21:29:20.485262Z","iopub.status.idle":"2026-02-12T21:29:20.615981Z","shell.execute_reply.started":"2026-02-12T21:29:20.485213Z","shell.execute_reply":"2026-02-12T21:29:20.615401Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"#### Training\nDesign and train a ffNN to deal with the “xview_recognition” classification task.","metadata":{"id":"diNBB3qy-AM2"}},{"cell_type":"code","source":"import json\n\n# Load database\njson_file = './xview_recognition/xview_ann_train.json'\nwith open(json_file) as ifs:\n    json_data = json.load(ifs)\nifs.close()","metadata":{"ExecuteTime":{"end_time":"2024-10-26T00:00:21.416449Z","start_time":"2024-10-26T00:00:21.311510Z"},"editable":true,"id":"Orto292C-AM3","slideshow":{"slide_type":""},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T21:29:20.616855Z","iopub.execute_input":"2026-02-12T21:29:20.617546Z","iopub.status.idle":"2026-02-12T21:29:20.693715Z","shell.execute_reply.started":"2026-02-12T21:29:20.617507Z","shell.execute_reply":"2026-02-12T21:29:20.692895Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import numpy as np\n\ncounts = dict.fromkeys(categories.values(), 0)\nanns = []\nfor json_img, json_ann in zip(json_data['images'].values(), json_data['annotations'].values()):\n    image = GenericImage(json_img['filename'])\n    image.tile = np.array([0, 0, json_img['width'], json_img['height']])\n    obj = GenericObject()\n    obj.bb = (int(json_ann['bbox'][0]), int(json_ann['bbox'][1]), int(json_ann['bbox'][2]), int(json_ann['bbox'][3]))\n    obj.category = json_ann['category_id']\n    # Resampling strategy to reduce training time\n    counts[obj.category] += 1\n    image.add_object(obj)\n    anns.append(image)\nprint(counts)","metadata":{"ExecuteTime":{"end_time":"2024-10-26T00:00:22.874518Z","start_time":"2024-10-26T00:00:22.204948Z"},"id":"4GjFLHs4-AM4","outputId":"5581df22-d4e9-42ac-9f94-061fd8c7acd9","trusted":true,"execution":{"iopub.status.busy":"2026-02-12T21:29:20.694813Z","iopub.execute_input":"2026-02-12T21:29:20.695202Z","iopub.status.idle":"2026-02-12T21:29:21.087312Z","shell.execute_reply.started":"2026-02-12T21:29:20.695171Z","shell.execute_reply":"2026-02-12T21:29:21.086442Z"}},"outputs":[{"name":"stdout","text":"{'Cargo plane': 635, 'Small car': 3324, 'Bus': 1768, 'Truck': 2210, 'Motorboat': 1069, 'Fishing vessel': 706, 'Dump truck': 1236, 'Excavator': 789, 'Building': 3594, 'Helipad': 111, 'Storage tank': 1469, 'Shipping container': 1523, 'Pylon': 312}\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nanns_train, anns_valid = train_test_split(anns, test_size=0.1, random_state=1, shuffle=True)\nprint('Number of training images: ' + str(len(anns_train)))\nprint('Number of validation images: ' + str(len(anns_valid)))","metadata":{"ExecuteTime":{"end_time":"2024-10-26T00:00:23.656800Z","start_time":"2024-10-26T00:00:23.123245Z"},"id":"NriAECvS-AM6","trusted":true,"execution":{"iopub.status.busy":"2026-02-12T21:29:21.088304Z","iopub.execute_input":"2026-02-12T21:29:21.088598Z","iopub.status.idle":"2026-02-12T21:29:21.134893Z","shell.execute_reply.started":"2026-02-12T21:29:21.088564Z","shell.execute_reply":"2026-02-12T21:29:21.134284Z"}},"outputs":[{"name":"stdout","text":"Number of training images: 16871\nNumber of validation images: 1875\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Load architecture\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation, Flatten, Input, AveragePooling2D, MaxPooling2D, BatchNormalization, Dropout\nfrom tensorflow.keras.losses import CategoricalFocalCrossentropy\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.activations import swish\n\nmodel = Sequential()\nmodel.add(Input(shape=(224, 224, 3)))\nmodel.add(MaxPooling2D(pool_size=(3, 3))) # pooling to reduce amount of parameters\nmodel.add(Flatten())\n\n# Layer 1\nmodel.add(Dense(1024))\n#model.add(BatchNormalization())\nmodel.add(Activation('swish')) \n#model.add(Dropout(0.4)) \n\n# Layer 2\nmodel.add(Dense(512))\n#model.add(BatchNormalization())\nmodel.add(Activation('swish'))\n#model.add(Dropout(0.4))\n\n# Layer 3\nmodel.add(Dense(256))\n#model.add(BatchNormalization())\nmodel.add(Activation('swish'))\n#model.add(Dropout(0.4))\n\n# Output\nmodel.add(Dense(len(categories)))\nmodel.add(Activation('softmax'))\n\n# Scheduler & Compiler\nlr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n    initial_learning_rate=0.001, \n    decay_steps=40 * (len(anns_train) // 32)\n)\n\nmodel.compile(\n    optimizer = Adam(learning_rate=lr_schedule, beta_1=0.9, beta_2=0.999, epsilon=1e-8, amsgrad=True, clipnorm=1.0),\n    loss=CategoricalFocalCrossentropy(alpha=0.25, gamma=2.0),\n    metrics=['accuracy']\n)\nmodel.summary()","metadata":{"ExecuteTime":{"end_time":"2024-10-26T00:00:25.056806Z","start_time":"2024-10-26T00:00:24.261581Z"},"id":"BNkjbY2e-AM7","outputId":"47bde031-306f-464e-8e22-cc70a7fb7c67","trusted":true,"execution":{"iopub.status.busy":"2026-02-12T21:29:21.135722Z","iopub.execute_input":"2026-02-12T21:29:21.136124Z","iopub.status.idle":"2026-02-12T21:29:22.055748Z","shell.execute_reply.started":"2026-02-12T21:29:21.136101Z","shell.execute_reply":"2026-02-12T21:29:22.055062Z"}},"outputs":[{"name":"stderr","text":"I0000 00:00:1770931761.231052  102710 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15511 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m3\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9408\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │     \u001b[38;5;34m4,817,408\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ activation (\u001b[38;5;33mActivation\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ activation_1 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ activation_2 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m)             │         \u001b[38;5;34m1,677\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ activation_3 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9408</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,817,408</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ activation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ activation_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ activation_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,677</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ activation_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,983,309\u001b[0m (19.01 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,983,309</span> (19.01 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,983,309\u001b[0m (19.01 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,983,309</span> (19.01 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"#from tensorflow.keras.optimizers import Adam\n#\n## Learning rate is changed to 0.001\n#opt = Adam(learning_rate=lr_schedule, beta_1=0.9, beta_2=0.999, epsilon=1e-8, amsgrad=True, clipnorm=1.0)\n#model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])","metadata":{"ExecuteTime":{"end_time":"2024-10-26T00:00:25.467525Z","start_time":"2024-10-26T00:00:25.434068Z"},"id":"-aSlKtG6-AM7","trusted":true,"execution":{"iopub.status.busy":"2026-02-12T21:29:22.056586Z","iopub.execute_input":"2026-02-12T21:29:22.056802Z","iopub.status.idle":"2026-02-12T21:29:22.060016Z","shell.execute_reply.started":"2026-02-12T21:29:22.056780Z","shell.execute_reply":"2026-02-12T21:29:22.059308Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"from tensorflow.keras.callbacks import TerminateOnNaN, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n\n# Callbacks\nmodel_checkpoint = ModelCheckpoint('model.keras', monitor='val_accuracy', verbose=1, save_best_only=True)\n#reduce_lr = ReduceLROnPlateau('val_accuracy', factor=0.1, patience=10, verbose=1)\nearly_stop = EarlyStopping('val_accuracy', patience=40, verbose=1)\nterminate = TerminateOnNaN()\ncallbacks = [model_checkpoint, early_stop, terminate]","metadata":{"ExecuteTime":{"end_time":"2024-10-26T00:00:26.254555Z","start_time":"2024-10-26T00:00:26.243908Z"},"id":"GGAJEfpB-AM8","trusted":true,"execution":{"iopub.status.busy":"2026-02-12T21:29:22.060938Z","iopub.execute_input":"2026-02-12T21:29:22.061536Z","iopub.status.idle":"2026-02-12T21:29:22.073769Z","shell.execute_reply.started":"2026-02-12T21:29:22.061502Z","shell.execute_reply":"2026-02-12T21:29:22.073083Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def generator_images(objs, batch_size, do_shuffle=False):\n    while True:\n        if do_shuffle:\n            np.random.shuffle(objs)\n        groups = [objs[i:i+batch_size] for i in range(0, len(objs), batch_size)]\n        for group in groups:\n            images, labels = [], []\n            for (filename, obj) in group:\n                # Load image\n                img = load_geoimage(filename)\n                # regularization\n                img = img.astype(np.float32) / 255.0\n                images.append(img)\n                probabilities = np.zeros(len(categories))\n                probabilities[list(categories.values()).index(obj.category)] = 1\n                labels.append(probabilities)\n            images = np.array(images).astype(np.float32)\n            labels = np.array(labels).astype(np.float32)\n            yield images, labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T21:29:22.074587Z","iopub.execute_input":"2026-02-12T21:29:22.074842Z","iopub.status.idle":"2026-02-12T21:29:22.089081Z","shell.execute_reply.started":"2026-02-12T21:29:22.074820Z","shell.execute_reply":"2026-02-12T21:29:22.088553Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Generate the list of objects from annotations\nobjs_train = [(ann.filename, obj) for ann in anns_train for obj in ann.objects]\nobjs_valid = [(ann.filename, obj) for ann in anns_valid for obj in ann.objects]\n# Generators\nbatch_size = 32 # change to 32 to stabilize the gradients\ntrain_generator = generator_images(objs_train, batch_size, do_shuffle=True)\nvalid_generator = generator_images(objs_valid, batch_size, do_shuffle=False)","metadata":{"ExecuteTime":{"end_time":"2024-10-26T00:00:27.058834Z","start_time":"2024-10-26T00:00:27.022627Z"},"id":"Yht-QqUH-AM8","trusted":true,"execution":{"iopub.status.busy":"2026-02-12T21:29:22.092157Z","iopub.execute_input":"2026-02-12T21:29:22.093545Z","iopub.status.idle":"2026-02-12T21:29:22.114394Z","shell.execute_reply.started":"2026-02-12T21:29:22.093483Z","shell.execute_reply":"2026-02-12T21:29:22.113745Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"import math\nimport numpy as np\n\nprint('Training model')\nepochs = 42 # model still improving after 20 epochs -> stabilization\ntrain_steps = math.ceil(len(objs_train)/batch_size)\nvalid_steps = math.ceil(len(objs_valid)/batch_size)\nh = model.fit(train_generator, steps_per_epoch=train_steps, validation_data=valid_generator, validation_steps=valid_steps, epochs=epochs, callbacks=callbacks, verbose=1)\n# Best validation model\nbest_idx = int(np.argmax(h.history['val_accuracy']))\nbest_value = np.max(h.history['val_accuracy'])\nprint('Best validation model: epoch ' + str(best_idx+1), ' - val_accuracy ' + str(best_value))","metadata":{"ExecuteTime":{"start_time":"2024-10-26T00:00:27.913670Z"},"editable":true,"id":"TrfpdECs-AM9","jupyter":{"is_executing":true},"outputId":"21d89b78-d94c-442e-9bc2-517654c0b614","slideshow":{"slide_type":""},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T21:29:22.115358Z","iopub.execute_input":"2026-02-12T21:29:22.115649Z"}},"outputs":[{"name":"stdout","text":"Training model\nEpoch 1/42\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1770931763.456278  102803 service.cc:152] XLA service 0x792c8800c700 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1770931763.456317  102803 service.cc:160]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\nI0000 00:00:1770931763.647625  102803 cuda_dnn.cc:529] Loaded cuDNN version 91002\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m  3/528\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 63ms/step - accuracy: 0.0642 - loss: 0.9672","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1770931764.560019  102803 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m528/528\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.2808 - loss: 0.5038\nEpoch 1: val_accuracy improved from -inf to 0.37813, saving model to model.keras\n\u001b[1m528/528\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 107ms/step - accuracy: 0.2809 - loss: 0.5035 - val_accuracy: 0.3781 - val_loss: 0.2771\nEpoch 2/42\n\u001b[1m527/528\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.4109 - loss: 0.2597\nEpoch 2: val_accuracy improved from 0.37813 to 0.41600, saving model to model.keras\n\u001b[1m528/528\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 102ms/step - accuracy: 0.4109 - loss: 0.2597 - val_accuracy: 0.4160 - val_loss: 0.2643\nEpoch 3/42\n\u001b[1m207/528\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 91ms/step - accuracy: 0.4310 - loss: 0.2401","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"#### Validation\nCompute validation metrics.","metadata":{"editable":true,"id":"8IMMO_mT-AM9","slideshow":{"slide_type":""},"tags":[]}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n%matplotlib inline\n\ndef draw_confusion_matrix(cm, categories):\n    # Draw confusion matrix\n    fig = plt.figure(figsize=[6.4*pow(len(categories), 0.5), 4.8*pow(len(categories), 0.5)])\n    ax = fig.add_subplot(111)\n    cm = cm.astype('float') / np.maximum(cm.sum(axis=1)[:, np.newaxis], np.finfo(np.float64).eps)\n    im = ax.imshow(cm, interpolation='nearest', cmap=plt.colormaps['Blues'])\n    ax.figure.colorbar(im, ax=ax)\n    ax.set(xticks=np.arange(cm.shape[1]), yticks=np.arange(cm.shape[0]), xticklabels=list(categories.values()), yticklabels=list(categories.values()), ylabel='Annotation', xlabel='Prediction')\n    # Rotate the tick labels and set their alignment\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n    # Loop over data dimensions and create text annotations\n    thresh = cm.max() / 2.0\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], '.2f'), ha=\"center\", va=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\", fontsize=int(20-pow(len(categories), 0.5)))\n    fig.tight_layout()\n    plt.show()","metadata":{"id":"HAanJ-V0-AM1","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\nmodel.load_weights('model.keras')\ny_true, y_pred = [], []\nfor ann in anns_valid:\n    # Load image\n    image = load_geoimage(ann.filename)\n    for obj_pred in ann.objects:\n        # Generate prediction\n        warped_image = np.expand_dims(image, 0)\n        predictions = model.predict(warped_image, verbose=0)\n        # Save prediction\n        pred_category = list(categories.values())[np.argmax(predictions)]\n        pred_score = np.max(predictions)\n        y_true.append(obj_pred.category)\n        y_pred.append(pred_category)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\n# Compute the confusion matrix\ncm = confusion_matrix(y_true, y_pred, labels=list(categories.values()))\ndraw_confusion_matrix(cm, categories)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\n# Compute the accuracy\ncorrect_samples_class = np.diag(cm).astype(float)\ntotal_samples_class = np.sum(cm, axis=1).astype(float)\ntotal_predicts_class = np.sum(cm, axis=0).astype(float)\nprint('Mean Accuracy: %.3f%%' % (np.sum(correct_samples_class) / np.sum(total_samples_class) * 100))\nacc = correct_samples_class / np.maximum(total_samples_class, np.finfo(np.float64).eps)\nprint('Mean Recall: %.3f%%' % (acc.mean() * 100))\nacc = correct_samples_class / np.maximum(total_predicts_class, np.finfo(np.float64).eps)\nprint('Mean Precision: %.3f%%' % (acc.mean() * 100))\nfor idx in range(len(categories)):\n    # True/False Positives (TP/FP) refer to the number of predicted positives that were correct/incorrect.\n    # True/False Negatives (TN/FN) refer to the number of predicted negatives that were correct/incorrect.\n    tp = cm[idx, idx]\n    fp = sum(cm[:, idx]) - tp\n    fn = sum(cm[idx, :]) - tp\n    tn = sum(np.delete(sum(cm) - cm[idx, :], idx))\n    # True Positive Rate: proportion of real positive cases that were correctly predicted as positive.\n    recall = tp / np.maximum(tp+fn, np.finfo(np.float64).eps)\n    # Precision: proportion of predicted positive cases that were truly real positives.\n    precision = tp / np.maximum(tp+fp, np.finfo(np.float64).eps)\n    # True Negative Rate: proportion of real negative cases that were correctly predicted as negative.\n    specificity = tn / np.maximum(tn+fp, np.finfo(np.float64).eps)\n    # Dice coefficient refers to two times the intersection of two sets divided by the sum of their areas.\n    # Dice = 2 |A∩B| / (|A|+|B|) = 2 TP / (2 TP + FP + FN)\n    f1_score = 2 * ((precision * recall) / np.maximum(precision+recall, np.finfo(np.float64).eps))\n    print('> %s: Recall: %.3f%% Precision: %.3f%% Specificity: %.3f%% Dice: %.3f%%' % (list(categories.values())[idx], recall*100, precision*100, specificity*100, f1_score*100))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Testing\nTry to improve the results provided in the competition.","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\n\nanns = []\nroot_dir = './xview_recognition/'\ntest_dir = os.path.join(root_dir, 'xview_test')\nfor (dirpath, dirnames, filenames) in os.walk(test_dir):\n    for filename in filenames:\n        rel_dir = os.path.relpath(dirpath, root_dir)\n        clean_filename = os.path.join(rel_dir, filename)\n        image = GenericImage(clean_filename)\n        image.tile = np.array([0, 0, 224, 224])\n        obj = GenericObject()\n        obj.bb = (0, 0, 224, 224)\n        obj.category = os.path.basename(dirpath)\n        image.add_object(obj)\n        anns.append(image)\nprint('Number of testing images: ' + str(len(anns)))","metadata":{"id":"tJr_-xCt-AM-","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\nmodel.load_weights('model.keras')\npredictions_data = {\"images\": {}, \"annotations\": {}}\nfor idx, ann in enumerate(anns):\n    image_data = {\"image_id\": ann.filename.split('/')[-1], \"filename\": ann.filename, \"width\": int(ann.tile[2]), \"height\": int(ann.tile[3])}\n    predictions_data[\"images\"][idx] = image_data\n    # Load image\n    image = load_geoimage(ann.filename)\n    for obj_pred in ann.objects:\n        # Generate prediction\n        warped_image = np.expand_dims(image, 0)\n        predictions = model.predict(warped_image, verbose=0)\n        # Save prediction\n        pred_category = list(categories.values())[np.argmax(predictions)]\n        pred_score = np.max(predictions)\n        annotation_data = {\"image_id\": ann.filename.split('/')[-1], \"category_id\": pred_category, \"bbox\": [int(x) for x in obj_pred.bb]}\n        predictions_data[\"annotations\"][idx] = annotation_data","metadata":{"id":"TGs2zqfv-AM_","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nwith open('prediction.json', 'w') as f:\n    json.dump(predictions_data, f)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}